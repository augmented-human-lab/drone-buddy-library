<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

      <title>User Guide</title>
    
          <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="_static/theme.css " type="text/css" />
      
      <!-- sphinx script_files -->
        <script src="_static/documentation_options.js?v=5929fcd5"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="_static/theme-vendors.js"></script> -->
      <script src="_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="genindex.html" />
  <link rel="search" title="Search" href="search.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="index.html" class="home-link">
    
      <span class="site-name">Dronebuddy</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#welcome-to-dronebuddy-s-documentation">Contents:</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 ">
            
              <a href="dronebuddylib.html" class="reference internal ">Introduction</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="dronebuddylib.userguide.html" class="reference internal ">User Guide</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="dronebuddylib.atoms.html" class="reference internal ">Atomic Modules</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="dronebuddylib.models.html" class="reference internal ">Model Definitions</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="dronebuddylib.utils.html" class="reference internal ">Utility Functions</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="dronebuddylib.configurations.html" class="reference internal ">Configuration Settings</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="modules.html" class="reference internal ">Module Index</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="dronebuddylib.developerguide.html" class="reference internal ">Developer Guide</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
    
    <li>User Guide</li>
  </ul>
  

  <ul class="page-nav">
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section id="user-guide">
<h1>User Guide<a class="headerlink" href="#user-guide" title="Link to this heading">¶</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h2>
<p>The vision of DroneBuddy is to give the possibility to everyone to programme their intelligent drone by themselves adding all the features they wish. DroneBuddy provides all the basic building blocks so you can use them to make your drone fly.
Best thing about DroneBuddy is that it is an offline library, so you don’t need an active internet connection to carry on the tasks.</p>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading">¶</a></h2>
<p>DroneBuddy is just another python library that you can install. You can find the library at <a class="reference external" href="https://pypi.org/project/dronebuddylib/">https://pypi.org/project/dronebuddylib/</a></p>
<p>But before the installation DroneBuddy needs some prerequisites.</p>
<p>It is compulsory that you follow all the steps, so that they are installed correctly. These requirements are requested by some of the machine learning models that we use in DroneBuddy.</p>
<ol class="arabic simple">
<li><p>Python 3.10 or higher</p></li>
<li><p>Install visual C++ through <a class="reference external" href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">https://visualstudio.microsoft.com/visual-cpp-build-tools/</a>, when you are installing there is an option for CMake, select that.</p></li>
<li><p>Need to install rust, use <a class="reference external" href="https://www.rust-lang.org/tools/install">https://www.rust-lang.org/tools/install</a>. Follow all the instructions are here</p></li>
<li><p>Install CMake and add it to the system path as here</p></li>
<li><p>pip install setuptools-rust</p></li>
</ol>
<section id="installing-pre-requisites">
<h3>Installing Pre-requisites<a class="headerlink" href="#installing-pre-requisites" title="Link to this heading">¶</a></h3>
<div class="toctree-wrapper compound">
</div>
</section>
</section>
<section id="installing-visual-c">
<h2>Installing visual C++<a class="headerlink" href="#installing-visual-c" title="Link to this heading">¶</a></h2>
<section id="windows-guide">
<h3>Windows guide<a class="headerlink" href="#windows-guide" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Go to this link <a class="reference external" href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">https://visualstudio.microsoft.com/visual-cpp-build-tools/</a>.</p></li>
<li><p>Download Microsoft C++ Build Tools</p></li>
<li><p>Install it from the setup, once it is installed, you will get a pop up window, the is an option to install C++</p></li>
<li><p>Select Desktop development with C++</p></li>
</ol>
<p>Install this option.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note: This C++ version is required to install CMake which is required to install the face recognition model used to recognize the people that you know.</p>
</div>
</section>
<section id="install-cmake">
<h3>Install CMake<a class="headerlink" href="#install-cmake" title="Link to this heading">¶</a></h3>
<p>CMake is required to install the dlib library which is required to install the face-recognition model. The official guidelines to install can be found here. <a class="reference external" href="https://github.com/ageitgey/face_recognition/issues/175#issue-257710508">https://github.com/ageitgey/face_recognition/issues/175#issue-257710508</a></p>
<ol class="arabic simple">
<li><p>The windows installer can be found here <a class="reference external" href="https://cmake.org/download/">https://cmake.org/download/</a></p></li>
<li><p>Once you run it, make sure it is added to the system path. The path would look like C:Program FilesCMakebin</p></li>
</ol>
</section>
<section id="rust-installation">
<h3>Rust installation<a class="headerlink" href="#rust-installation" title="Link to this heading">¶</a></h3>
<p>Rust is required to install the intent recognition library that is needed to understand the commands given to the drone.</p>
<p>You can download Rust from here  <a class="reference external" href="https://www.rust-lang.org/tools/install">https://www.rust-lang.org/tools/install</a></p>
<p>The steps will differ from Windows to mac, so follow the steps on the website.</p>
<p>Windows installation is pretty straightforward</p>
</section>
<section id="mac-os-installation">
<h3>mac OS installation<a class="headerlink" href="#mac-os-installation" title="Link to this heading">¶</a></h3>
<p>If you get a permission error while installing , follow the <a class="reference external" href="https://stackoverflow.com/questions/45899815/could-not-write-to-bash-profile-when-installing-rust-on-macos-sierra">https://stackoverflow.com/questions/45899815/could-not-write-to-bash-profile-when-installing-rust-on-macos-sierra</a> steps given in the answer here.</p>
<p>Give a try using this not using sudo:
.. code-block:: bash</p>
<blockquote>
<div><p>curl <a class="reference external" href="https://sh.rustup.rs">https://sh.rustup.rs</a> -sSf | sh -s – –help</p>
</div></blockquote>
<p>If that works then probably you could try:
.. code-block:: bash</p>
<blockquote>
<div><p>curl <a class="reference external" href="https://sh.rustup.rs">https://sh.rustup.rs</a> -sSf | sh -s – –no-modify-path</p>
</div></blockquote>
<p>If the command with the –no-modify-path option works, you’ll have to manually update .bash_profile to include it in your path:
.. code-block:: bash</p>
<blockquote>
<div><p>source ~/.cargo/env</p>
</div></blockquote>
<p>Now you are all ready to install DroneBuddy.</p>
</section>
</section>
<section id="installing-dronebuddy">
<h2>Installing DroneBuddy<a class="headerlink" href="#installing-dronebuddy" title="Link to this heading">¶</a></h2>
<p>Installation is as any other library..</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>dronebuddylib
</pre></div>
</div>
<p>Now let’s get into detail regarding all the functionalities DroneBuddy offers.</p>
<p>Once dronebuddylib is installed,</p>
</section>
<section id="voice-recognition">
<h2>Voice Recognition<a class="headerlink" href="#voice-recognition" title="Link to this heading">¶</a></h2>
<section id="general">
<h3>General<a class="headerlink" href="#general" title="Link to this heading">¶</a></h3>
<p>Voice recognition, also known as speech recognition, is a technology that allows computers or machines to understand and interpret spoken language. It enables the conversion of spoken words into written text or commands that can be understood and processed by a computer.</p>
<p>Here’s a simplified explanation of how voice recognition works:</p>
<ol class="arabic simple">
<li><p>Audio Input: Voice recognition systems take audio input as their primary source of information. This audio input can be obtained from various sources, such as a microphone, recorded audio files, or even real-time streaming audio. In our case for us to command the drone we will take the input from our computer. There is one more possibility of using the stream from the drone itself, but this will be very messy, there will be a lot of background noise which would not do well with the accuracy. And also this will put too much strain on the drone, which will cause the drone to unnecessarily heat up and drain the battery.</p></li>
<li><p>Pre-processing: Before analyzing the audio, voice recognition systems often apply pre-processing techniques to enhance the quality of the input. This may involve removing background noise, normalizing the audio volume, or applying filters to improve the accuracy of recognition..</p></li>
<li><p>Acoustic Analysis: The audio input is then analyzed to extract various acoustic features. These features capture information about the sound, such as the frequency, intensity, and duration of different speech sounds.</p></li>
<li><p>Acoustic Model: An acoustic model is a trained statistical model that associates the observed acoustic features with the corresponding speech sounds or phonemes. It helps identify and differentiate between different speech sounds in the audio.</p></li>
<li><p>Language Model: A language model helps in understanding the context and improving the accuracy of recognition. It uses probabilistic methods to predict the most likely sequence of words or phrases based on the acoustic input. Language models incorporate grammar rules, vocabulary, and statistical language patterns to generate the most probable textual output.</p></li>
<li><p>Speech Recognition: In this step, the acoustic model and the language model work together. The observed acoustic features are compared with the models to determine the most likely word sequence that corresponds to the input speech. This involves matching the acoustic patterns against a vast database of pre-recorded speech samples.</p></li>
<li><p>Output Generation: The recognized words or phrases are generated as output, typically in the form of written text. This output can be further processed or used as input for various applications, such as transcription services, voice assistants, or voice-controlled systems.</p></li>
</ol>
<p>It’s important to note that voice recognition technology is continually evolving and improving. However, it can still face challenges in accurately recognizing speech due to factors like background noise, accents, speech impediments, and variations in pronunciation.</p>
<p>Voice recognition has numerous applications, including dictation software, transcription services, virtual assistants, interactive voice response systems, and more. It offers a convenient and efficient way for humans to interact with computers and devices through spoken language.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html">Supported models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#vosk">VOSK</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#general">General</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#how-to-use-with-drone-buddy">How to use with drone buddy</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#required-parameters">Required parameters</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#google-speech-recognition">Google Speech Recognition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#id1">General</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#integration-in-dronebuddy">Integration in Dronebuddy</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#using-google-speech-recognition-for-command-control">Using Google Speech Recognition for Command Control</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicerecognition.html#important-considerations">Important Considerations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="intent-recognition">
<h2>Intent Recognition<a class="headerlink" href="#intent-recognition" title="Link to this heading">¶</a></h2>
<section id="id1">
<h3>General<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<p>Intent recognition, also known as intent detection or intent classification, is a technique used in natural language processing (NLP) to identify the intention or purpose behind a given piece of text. It helps computers understand the meaning and intention behind human language, enabling them to respond appropriately.</p>
<p>Here’s a simplified explanation of how intent recognition works:</p>
<ol class="arabic simple">
<li><p>Text Input: Intent recognition takes a text input, typically a user’s query, command, or statement, as its primary source of information. This text input can be in the form of a sentence, a phrase, or even a single word.</p></li>
<li><p>Pre-processing: Before analyzing the text, intent recognition systems often perform pre-processing steps. These steps may include removing punctuation, converting the text to lowercase, removing stop words (common words like “the,” “is,” etc.), and handling any other necessary formatting or cleaning.</p></li>
<li><p>Feature Extraction: Intent recognition systems extract relevant features from the pre-processed text. These features can include words, word combinations (n-grams), part-of-speech tags, syntactic structures, or any other linguistic information that helps capture the meaning and context of the text.</p></li>
<li><p>Training Data: Intent recognition models require training data to learn how to classify different intents. Training data consists of labeled examples where each text input is associated with a specific intent. For instance, if the system is designed to recognize user commands, the training data might contain examples like “Play a song,” “Stop the video,” etc., along with their corresponding intents.</p></li>
<li><p>Machine Learning Model: Machine learning techniques, such as supervised learning, are commonly used for intent recognition. A model is trained on the labeled training data, learning the patterns and relationships between the extracted features and the corresponding intents.</p></li>
<li><p>Intent Classification: Once the model is trained, it can be used to predict the intent of new, unseen text inputs. The trained model takes the extracted features from the new text input and applies the learned patterns to classify it into one or more predefined intents. The predicted intent represents the underlying meaning or purpose of the text.</p></li>
<li><p>Output Generation: The predicted intent is generated as output, providing information about the user’s intention. This output can be further processed to trigger specific actions, retrieve relevant information, or provide appropriate responses based on the recognized intent.</p></li>
</ol>
<p>Intent recognition is widely used in various applications, such as chatbots, virtual assistants, customer support systems, and voice-controlled devices. It enables these systems to understand user queries, commands, or statements and respond accordingly, providing a more interactive and personalized user experience.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>It’s important to note that the accuracy of intent recognition depends on the quality and diversity of the training data, the effectiveness of feature extraction techniques, and the robustness of the machine learning model employed.</p>
</div>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html">Supported models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#snips-nlu">SNIPS NLU</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#integration-in-dronebuddy">Integration in DroneBuddy</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#install-setuptools-rust">Install setuptools-rust</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#install-required-dependencies-for-snips-nlu">Install required dependencies for Snips NLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#install-snips-nlu">Install Snips NLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#create-data-set">Create data set</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#intents">Intents</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#entities">Entities</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#using-the-nlu">Using the NLU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#chat-gpt">Chat GPT</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#install-required-dependencies-for-chat-gpt">Install required dependencies for chat GPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#id1">Install Snips NLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#id2">Integration in DroneBuddy</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#example-of-chatgpt-integration-in-dronebuddy">Example of ChatGPT Integration in DroneBuddy</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.intentrecognition.html#important-considerations">Important Considerations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="face-recognition">
<h2>Face Recognition<a class="headerlink" href="#face-recognition" title="Link to this heading">¶</a></h2>
<section id="id2">
<h3>General<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<p>Face recognition is a technology that identifies or verifies a person’s identity by analyzing their facial features. It is commonly used in various applications, such as security systems, access control, biometric authentication, and surveillance.</p>
<p>Here’s a simplified explanation of how face recognition works:</p>
<ol class="arabic simple">
<li><p>Face Detection: The face recognition process begins with face detection, where an algorithm locates and detects faces within an image or a video stream. This involves analyzing the visual information to identify areas that potentially contain faces.</p></li>
<li><p>Face Alignment: Once faces are detected, face alignment techniques are applied to normalize the face’s position and orientation. This step helps ensure that the subsequent analysis focuses on the important facial features, regardless of slight variations in pose or facial expression.</p></li>
<li><p>Feature Extraction: In this step, the facial features are extracted from the aligned face image. Various algorithms, such as eigenfaces, local binary patterns, or deep neural networks, are used to analyze the unique characteristics of the face and represent them as numerical feature vectors. These feature vectors capture information about the geometry, texture, and other distinctive aspects of the face.</p></li>
<li><p>Training: Face recognition systems require training with labeled examples to learn how to identify individuals. During the training phase, the system learns to map the extracted facial features to specific identities. It builds a model or database that represents the known faces and their corresponding features.</p></li>
<li><p>Face Matching: When a face needs to be recognized, the system compares the extracted features of the query face with the features stored in the trained model or database. It calculates the similarity or distance between the feature vectors to determine the closest matches. The recognition algorithm uses statistical methods or machine learning techniques to make this comparison.</p></li>
<li><p>Recognition Decision: Based on the calculated similarity scores, the system makes a recognition decision. If a sufficiently close match is found in the database, the face is recognized as belonging to a specific individual. Otherwise, if the match is not close enough, the system may classify the face as unknown.</p></li>
</ol>
<p>Face recognition systems can be designed for different purposes, such as one-to-one verification (confirming whether a person is who they claim to be) or one-to-many identification (finding a person’s identity from a large database). The level of accuracy and performance can vary based on factors such as image quality, variations in lighting and pose, and the quality of the face recognition algorithm used.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html">Supported models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#face-recognition">Face-recognition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#dlib">dlib</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#why-do-we-need-dlib">Why do we need dlib</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#install">Install</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#for-windows-installation">For windows installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#resources">Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#integration-in-dronebuddy">Integration in DroneBuddy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#add-faces-to-the-memory">Add faces to the memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.facerecognition.html#recognize-faces">Recognize faces</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="object-detection">
<h2>Object detection<a class="headerlink" href="#object-detection" title="Link to this heading">¶</a></h2>
<section id="id3">
<h3>General<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>Object detection is a computer vision technique that involves locating and classifying objects within images or video frames. It goes beyond simple image classification by not only identifying the presence of objects but also providing information about their precise locations within the scene.</p>
<p>Here’s a simplified explanation of how object detection works:</p>
<ol class="arabic simple">
<li><p>Image Preprocessing: The object detection process typically starts with some preprocessing steps, such as resizing, normalization, or adjusting the image’s color channels. These steps help prepare the image for further analysis and improve the accuracy of object detection algorithms.</p></li>
<li><p>Feature Extraction: Object detection algorithms employ various techniques to extract features from the image that represent meaningful patterns or characteristics of objects. Commonly used methods include the Histogram of Oriented Gradients (HOG), Scale-Invariant Feature Transform (SIFT), or Convolutional Neural Networks (CNNs). These features capture essential information about the objects’ appearance, shape, and texture.</p></li>
<li><p>Object Localization: Object localization involves determining the spatial coordinates of objects within the image. This is typically achieved by identifying the boundaries of objects through techniques like edge detection, contour detection, or region proposal algorithms. The result is a bounding box that tightly encloses each detected object.</p></li>
<li><p>Classification: Once objects are localized, object detection algorithms assign class labels to each detected object. Classification can be performed using machine learning techniques like Support Vector Machines (SVMs), Random Forests, or CNNs. The model has been previously trained on a dataset with annotated examples of different object classes, allowing it to learn to recognize and classify objects accurately.</p></li>
<li><p>Post-processing: In the post-processing step, the object detection algorithm refines the results to improve the overall quality. This may involve filtering out detections based on their confidence scores, removing overlapping or redundant bounding boxes, or applying heuristics to handle edge cases or false positives.</p></li>
</ol>
<p>Object detection can be applied to a wide range of applications, including autonomous driving, surveillance systems, robotics, and image understanding tasks. The accuracy and performance of object detection algorithms depend on factors such as the quality of training data, the choice of features and algorithms, and the computational resources available.</p>
<p>It’s important to note that object detection is an active area of research, and there are several algorithms and frameworks available to perform object detection, including Faster R-CNN, YOLO (You Only Look Once), and SSD (Single Shot MultiBox Detector). These algorithms differ in their trade-offs between accuracy and speed, making them suitable for different use cases.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html">Supported models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#yolo">YOLO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#install-required-dependencies-for-yolo">Install required dependencies for YOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#id1">Install required dependencies for YOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#how-to-use">How to use</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#mediapipe">MediaPipe</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#integration-in-dronebuddy">Integration in DroneBuddy</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#example-of-mediapipe-integration-in-dronebuddy">Example of MediaPipe Integration in DroneBuddy</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#install-required-dependencies-for-snips-nlu">Install required dependencies for Snips NLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#id2">Install required dependencies for Snips NLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#id3">How to use</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.objectdetection.html#important-considerations">Important Considerations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="voice-generation">
<h2>Voice generation<a class="headerlink" href="#voice-generation" title="Link to this heading">¶</a></h2>
<section id="id4">
<h3>General<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<p>Text-to-speech (TTS) is a technology that converts written text into spoken words. It enables computers and other devices to generate human-like speech by processing and synthesizing text input.</p>
<p>Here’s a simplified explanation of how text-to-speech works:</p>
<p>Text Processing: The input text is processed to remove any unwanted characters, punctuation, or formatting. It may also involve tokenization, which breaks the text into smaller units such as words or phonemes for further analysis.</p>
<p>Linguistic Analysis: The processed text is analyzed to extract linguistic features and interpret the meaning of the words and sentences. This analysis may involve tasks like part-of-speech tagging, syntactic parsing, and semantic understanding to ensure accurate pronunciation and intonation.</p>
<p>Speech Synthesis: Once the linguistic analysis is complete, the text is transformed into speech signals. This is typically done through speech synthesis algorithms that generate the corresponding waveforms based on the linguistic information.</p>
<p>Concatenative Synthesis: One approach is concatenative synthesis, where pre-recorded segments of human speech (called “units”) are stored in a database. These units are selected and concatenated together to form the synthesized speech. This method can produce natural-sounding results but requires a large database of recorded speech.</p>
<p>Formant Synthesis: Another approach is formant synthesis, which generates speech by modeling the vocal tract and manipulating its resonances (formants). By controlling the formants’ frequencies and amplitudes, synthetic speech is produced. Formant synthesis allows for more control over the speech output but may sound less natural compared to concatenative synthesis.</p>
<p>Parametric Synthesis: Parametric synthesis uses mathematical models to represent speech characteristics. It employs a set of parameters, such as pitch, duration, and spectral envelope, to generate speech waveforms. Parametric synthesis allows for efficient storage and customization of speech but may require additional processing to sound natural.</p>
<p>Voice and Prosody: TTS systems often have multiple voices available, each representing a different speaker or style. The selected voice determines the characteristics of the generated speech, such as pitch, intonation, and accent. Prosody refers to the rhythm, stress, and intonation patterns in speech, and TTS systems incorporate prosodic rules to make the synthesized speech sound more natural and expressive.</p>
<p>Output: The final output of the TTS system is the synthesized speech, which can be played back through speakers, headphones, or integrated into applications, devices, or services for various purposes like accessibility, voice assistants, audiobooks, or interactive systems.</p>
<p>TTS technology has made significant advancements in recent years, leveraging machine learning and deep neural networks to improve the naturalness and quality of synthesized speech. Deep learning models, such as WaveNet and Tacotron, have demonstrated impressive results in generating high-fidelity and expressive speech.</p>
<p>It’s important to note that TTS systems require training on large datasets of recorded speech to produce accurate and natural-sounding results. The quality and performance of TTS systems can vary depending on the available resources, language, voice dataset, and synthesis techniques used.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="dronebuddylib.userguide.voicegeneration.html">Supported models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dronebuddylib.userguide.voicegeneration.html#pyttsx3">Pyttsx3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicegeneration.html#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="dronebuddylib.userguide.voicegeneration.html#how-to-use-with-dronebuddy">How to use with DroneBuddy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
</section>
</section>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
</ul><div class="footer" role="contentinfo">
      &#169; Copyright 2023, NUS.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 7.2.2 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.8.0.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>